{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    Trainer,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","import os\n","import json\n","import pandas as pd\n","import gc"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from accelerate import PartialState\n","device_string = PartialState().process_index"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 2\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 2\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {'':device_string}"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model_name = \"NingLab/eCeLLM-L\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_response(prompt):\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        quantization_config=bnb_config,\n","        device_map=device_map\n","    )\n","    model.config.use_cache = False\n","    model.config.pretraining_tp = 1\n","\n","    # Load LLaMA tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","\n","    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=4000)\n","    result = pipe(f\"{prompt}'### Output':\")\n","    result = result[0]['generated_text']\n","\n","    del model\n","    del tokenizer\n","    gc.collect()\n","    gc.collect()\n","    return result"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import json\n","\n","with open(\"./data/Clustering.json\", \"r\") as f:\n","    clustering = json.load(f)\n","\n","with open(\"./data/Preferences.json\", \"r\") as f:\n","    Preferences = json.load(f)\n","    \n","with open(\"./data/Insurance.json\", \"r\") as f:\n","    Insurance = json.load(f)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import random\n","#Create a prompt\n","keys_cluster = list(clustering.keys())\n","\n","get_random_keys = random.sample(keys_cluster, 3)\n","\n","values = []\n","for keys in get_random_keys:\n","    values.append(clustering[keys][\"tags\"])\n","\n","final = []\n","for subarray in values:\n","    final.extend(subarray)\n","\n","final = ', '.join(final)\n","\n","preferences = []\n","for key in Preferences.keys():\n","    preferences.append(key)\n","\n","preferences = ', '.join(preferences)\n","\n","insurances = []\n","for key in Insurance.keys():\n","    insurances.append(key)\n","\n","insurances = ', '.join(insurances)\n","\n","\n","prompt3 = f\"What is the next product that the user would be interested in? The user's interests are '{final}'. The possible options are '{insurances}'\"\n","prompt2 = f\"What is the next product that the user would be interested in? The user's interests are '{final}'.\" \n","prompt = f\"What is the next product that the user would be interested in? The user's interests are '{final}'. The possible options are '{preferences}'\"\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program Files\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60ae40ef3465482b85954448bb9f2553","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"]}],"source":["with open(\"./output/prompt.txt\", \"w\") as f:\n","    f.write(prompt)\n","    f.write(\"\\n\\n\")\n","    f.write(\"Output: \\n\" + get_response(prompt).split(\"Output\")[1])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22ff324ce5694c4b96122932213f232f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["with open(\"./output/prompt2.txt\", \"w\") as f:\n","    f.write(prompt)\n","    f.write(\"\\n\\n\")\n","    f.write(\"Output: \\n\" + get_response(prompt2).split(\"Output\")[1])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a98a02ada37a41e699a90e28381a729d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["with open(\"./output/prompt3.txt\", \"w\") as f:\n","    f.write(prompt)\n","    f.write(\"\\n\\n\")\n","    f.write(\"Output: \\n\" + get_response(prompt3).split(\"Output\")[1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":2}
